{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial we will use the basic functionallity of EMCEE (the MCMC software) to fit a quadratic function with some noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "import numpy as np\n",
    "import emcee\n",
    "import matplotlib.pyplot as pl\n",
    "from matplotlib.pyplot import cm \n",
    "import matplotlib.mlab as mlab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we make our synthetic data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def q(a,b,c,x):\n",
    "    quad = a*x**2. + b*x + c\n",
    "    return quad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = np.linspace(-1,1,101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quad = q(1.,0.,-0.2,x)\n",
    "noise = np.random.normal(0.0, 0.1, quad.shape)\n",
    "noisy = quad + noise\n",
    "pl.plot (x,noisy,\"k.\")\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we set the hyperparamters required to run emcee:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nwalkers = 20\n",
    "niter = 10\n",
    "init_dist = [(-2.,0.),(-0.5,0.5),(-0.5,0.)]\n",
    "ndim = len(init_dist)\n",
    "sigma = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will set up some prior information. If we know something about our system, say from spectra, we can inform our model by setting priors. Here we will set uninformed priors, but you can have informed priors such as gaussian priors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "priors = [(-4.,4.),(-1.,1.),(-1.,1.)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate an initial guess for all the parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rpars(init_dist):\n",
    "    return [np.random.rand() * (i[1]-i[0]) + i[0] for i in init_dist]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instate the priors such that if a parameter falls outside the prior range, the value will be set to negative infinity and the model will not be accepted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lnprior(priors, values):\n",
    "    \n",
    "    lp = 0.\n",
    "    for value, prior in zip(values, priors):\n",
    "        if value >= prior[0] and value <= prior[1]:\n",
    "            lp+=0\n",
    "        else:\n",
    "            lp+=-np.inf \n",
    "    return lp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will specify our log probability function, which will include our figure of merit computation. Here, z will be the array of values returned by the mcmc algorithm. The value this function returns is the log probability value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lnprob(z):\n",
    "    \n",
    "    lnp = lnprior(priors,z)\n",
    "    if not np.isfinite(lnp):\n",
    "            return -np.inf\n",
    "\n",
    "    # make a model using the values the sampler generated\n",
    "    model = q(z[0],z[1],z[2],x)\n",
    "\n",
    "    # use chi^2 to compare the model to the data:\n",
    "    chi2 = 0.\n",
    "    for i in range (len(x)):\n",
    "            chi2+=((noisy[i]-model[i])**2)/(sigma**2)\n",
    "\n",
    "    # calculate lnp\n",
    "    lnprob = -0.5*chi2 + lnp\n",
    "\n",
    "    return lnprob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will set the run function which will combine all our computations and plot histograms of the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run(init_dist, nwalkers, niter, ndim):\n",
    "\n",
    "    # Generate initial guesses for all parameters for all chains\n",
    "    p0 = np.array([rpars(init_dist) for i in xrange(nwalkers)])\n",
    "\n",
    "    # Generate the emcee sampler. Here the inputs provided include the \n",
    "    # lnprob function. With this setup, the first parameter\n",
    "    # in the lnprob function is the output from the sampler (the paramter \n",
    "    # positions).\n",
    "    sampler = emcee.EnsembleSampler(nwalkers,ndim,lnprob)\n",
    "\n",
    "    pos, prob, state = sampler.run_mcmc(p0, niter)\n",
    "\n",
    "    for i in range(ndim):\n",
    "        pl.figure()\n",
    "        y = sampler.flatchain[:,i]\n",
    "        n, bins, patches = pl.hist(y, 200, normed=1, color=\"b\", alpha=0.45)\n",
    "        pl.title(\"Dimension {0:d}\".format(i))\n",
    "        \n",
    "        mu = np.average(y)\n",
    "        sigma = np.std(y)       \n",
    "        print \"mu,\", \"sigma = \", mu, sigma\n",
    "\n",
    "        bf = mlab.normpdf(bins, mu, sigma)\n",
    "        l = pl.plot(bins, bf, 'k--', linewidth=2.0)\n",
    "\n",
    "    pl.show()\n",
    "    return pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "niter=10\n",
    "pos = run(init_dist, nwalkers, niter, ndim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color=cm.rainbow(np.linspace(0,1,nwalkers))\n",
    "for i,c in zip(range(nwalkers),color):\n",
    "    \n",
    "    model = pos[-1-i,0]*x**2 + pos[-1-i,1]*x + pos[-1-i,2]\n",
    "    \n",
    "    pl.plot(x,model,c=c)\n",
    "    \n",
    "pl.plot(x,noisy,\"k.\")\n",
    "pl.xlabel(\"x\")\n",
    "pl.ylabel(\"f(x)\")\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets update the number of iterations to see how this affects our parameter values (this may take a while to run):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "niter = 1000\n",
    "\n",
    "pos = run(init_dist, nwalkers, niter, ndim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color=cm.rainbow(np.linspace(0,1,nwalkers))\n",
    "for i,c in zip(range(nwalkers),color):\n",
    "    \n",
    "    model = pos[-1-i,0]*x**2 + pos[-1-i,1]*x + pos[-1-i,2]\n",
    "    \n",
    "    pl.plot(x,model,c=c)\n",
    "    \n",
    "pl.plot(x,noisy,\"k.\")\n",
    "pl.xlabel(\"x\")\n",
    "pl.ylabel(\"f(x)\")\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
